import pandas as pd
from dataclasses import dataclass
from joblib import Parallel, delayed

from data_simulation.simulate_data import Experiment, ExperimentParameter
from cluster_initialization.init_class import Cluster_initialization, Cluster_Initialization_Parameter
from em_algorithm.em_class import EM, EM_Parameter

from miscellaneous.parameter import Parameter, nested_dataclass
from miscellaneous.exception_handler_decorator import catch_exceptions
from miscellaneous.logger import Logger

@dataclass
class Parallel_Parameter(Parameter):
    n_jobs: int = 10
    verbose: int = 11


@nested_dataclass
class MCParameter(Parameter):
    N_experiments: int = 5
    parallel: Parallel_Parameter = Parallel_Parameter()
    experiment: ExperimentParameter = ExperimentParameter()
    em: EM_Parameter = EM_Parameter()
    cluster_init: Cluster_Initialization_Parameter = Cluster_Initialization_Parameter()



class MC:
    logger = Logger.create_logger()
    def __init__(self, **kwargs) -> None:
        self.params =MCParameter(**kwargs)  #**update(MCParameter().get_dict(), kwargs)
        self.logger.info(f"Initialization of a MC object with the following parameters has been carried out.\n{self.params}")

        # processed results of the mc run:
        self.model_data = None
        self.df_results = None
    

    def load_experiments(self, experiments: list[Experiment]):
        """load experiments created outside of this object

        Args:
            experiments (list[Experiment]): list of `Experiment` objects
        """
        self.experiments = experiments
        self.N_experiments = len(experiments)
    

    def simulate_experiments(self, experiment_params: dict = None, N_experiments: int =None):
        """Simulates experiments which then will be clustered by `self.run`.\\ 
        If no `Args` are given, the default parameters will be used for the generation.

        Args:
            experiment_params (dict, optional): params used to create `Experiment` objects. Defaults to None.
            N_experiments (int, optional): Number of `Experiment` objects created. Defaults to None.
        """
        if experiment_params is not None:
            self.experiment_params = experiment_params
        if N_experiments is not None:
            self.N_experiments = N_experiments
        self.experiments = [Experiment(**self.params.experiment.get_dict()) for i in range(self.params.N_experiments)]


    @catch_exceptions(logger)
    def run(self):
        self.logger.info(f"Starting the clustering of experimental data batch with init_routine {self.params.cluster_init.init_routine}.\n")
        parallel_results = self._parallel_clustering()

        self.logger.info(f"\nClustering successfull; starting processing of results")
        self.df_results, self.model_data = self._process_parallel_results(parallel_results)

        self.logger.info(f"Processing successfull - logger will be closed - END\n\n")
        Logger.close_logger()


    @catch_exceptions(logger)
    def _parallel_clustering(self):
        """uses `joblib` library to perform the clustering of each experiment in parallel
        """
        parallel = Parallel(**self.params.parallel.get_dict()) 
        parallel_results = parallel(delayed(self._cluster_experiment)(
            self.params.cluster_init.get_dict(), 
            self.params.em.get_dict(), 
            i,
            experiment.df
            ) for i, experiment in enumerate(self.experiments)
        )
        self.logger.info(f"Clustering with init routine {self.params.cluster_init.init_routine} has finished.")
        return parallel_results

    @staticmethod
    @catch_exceptions(logger)
    def _cluster_experiment(cluster_init_params: dict, em_params: dict, i: int, df_experiment: pd.DataFrame):
        """Method which each parallel process calls

        Args:
            init_routine (str): determine which Cluster Intialization method is used for the starting values
            cluster_init_params (dict): parameters passed to `Cluster_Initialization`
            em_params (dict): parameters passed to `EM`
            i (int): index for each set of points the MC run analyses
            df_experiment (pd.DataFrame): the dataframe containing the points to be clustered

        Returns:
            (dict): contains stats and results from the executed EM-algorithm
        """
        logger = Logger.reload_logger()  # needed because joblib sometimes messes things up
        cluster_init = Cluster_initialization(df_experiment, **cluster_init_params)
        cluster_init.sample()
        em = EM(**em_params)
        em.run(df_experiment, cluster_init=cluster_init)
        logger.info(f"Experiment {i} has finished.")
        return em.results.get_dict()

    
    @catch_exceptions(logger)
    def _process_parallel_results(self, parallel_results):
        """wrapper for processing results generated by `self._parallel_clustering()`
        """
        merged_results = self._merge_parallel_results(parallel_results)
        df_results, model_data = self._create_df_from_results(merged_results) 
        model_data = self._include_data_from_true_models(model_data)
        # include clusternumbers into df_results:
        df_results = self._include_true_clusternumber(df_results, model_data)
        df_results = self._include_model_clusternumber(df_results, model_data)
        return df_results, model_data


    @catch_exceptions(logger)
    def _merge_parallel_results(self, parallel_results):
        merged_result = {key: [] for key in parallel_results[0].keys()}
        merged_result["dataset"] = []
        for dataset_idx, result in enumerate(parallel_results):
            if result is None:
                self.logger.warning(f"Custering of dataset {dataset_idx} has failed.")
                continue
            merged_result["dataset"] = merged_result["dataset"] + [dataset_idx for _i in range(len(result["iterations"]))]
            for feature_name, list_ in result.items():
                merged_result[feature_name] = merged_result[feature_name] + list_
        return merged_result
    
    @catch_exceptions(logger)
    def _create_df_from_results(self, merged_results, non_df_cols = ["inferred_mixtures", "starting_values"]):
        df_result = pd.DataFrame.from_dict({key: val for key, val in merged_results.items() if key not in non_df_cols})
        df_result = df_result.reset_index().rename(columns={"index": "model_idx"})
        return df_result, {key: val for key, val in merged_results.items() if key in non_df_cols}

    @catch_exceptions(logger)
    def _include_data_from_true_models(self, model_data):
        model_data["datasets"] = [experiment.df for experiment in self.experiments]
        model_data["true_models"] = [experiment.get_distr_params_df() for experiment in self.experiments]
        model_data["init_routine"] = self.params.cluster_init.init_routine
        return model_data

    @staticmethod   
    def _include_true_clusternumber(df_results, model_data):
        for dataset in df_results.dataset.unique():
            idx = df_results.dataset == dataset
            df_results.loc[idx, "True_N_Cluster"] = len(model_data["datasets"][dataset].cluster.unique())
        df_results["True_N_Cluster"] = df_results["True_N_Cluster"].astype(int)
        return df_results

    @staticmethod
    def _include_model_clusternumber(df_results, model_data):
        N_cluster_list = []
        for model in df_results.itertuples():
            N_cluster_list.append(len(model_data["inferred_mixtures"][model.model_idx])//4)
        df_results["N_cluster"] = N_cluster_list
        return df_results

